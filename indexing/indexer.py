"""
ì‘ì„±ì : kp
ì‘ì„±ì¼ : 2025-05-18 (ìˆ˜ì •: 2025-05-19)
ëª©ì  : PDF ë¬¸ì„œ ì²­í¬ ë° ë²¡í„° ì„ë² ë”© í›„ ChromaDB ì €ì¥
ë‚´ìš© : PyMuPDFLoader + RecursiveCharacterTextSplitter + HuggingFaceEmbeddings ê¸°ë°˜ìœ¼ë¡œ
       PDF ë¬¸ì„œë¥¼ ì½ê³  í°íŠ¸ í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ëœ ì„¹ì…˜ ì œëª©ì„ í¬í•¨í•˜ì—¬ chromaì— ì €ì¥
"""

import os
import glob
from typing import List, Dict, Any
import shutil
import fitz # PyMuPDF

# Langchain ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (í™˜ê²½ì— ë”°ë¼ langchain_community ë“±ìœ¼ë¡œ ë³€ê²½ë  ìˆ˜ ìˆìŒ)
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv

load_dotenv()

# ğŸ“ ì„¤ì •
PDF_DIR = "./data/claude/"  # indexer.py íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
CHROMA_DIR = "./vectorstore/chroma_claude" # indexer.py íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 250
CHUNK_OVERLAP = 50
# í°íŠ¸ ê¸°ë°˜ ì œëª© ì¶”ë¡ ì„ ìœ„í•œ ì„ê³„ê°’
TITLE_FONT_SIZE_MIN_DIFFERENCE = 1.5 # ì¼ë°˜ í…ìŠ¤íŠ¸ë³´ë‹¤ ìµœì†Œ ì´ë§Œí¼ ì»¤ì•¼ ì œëª©ìœ¼ë¡œ ê°„ì£¼ (ì ˆëŒ€ê°’)
TITLE_FONT_SIZE_MIN_RATIO = 1.15    # ì¼ë°˜ í…ìŠ¤íŠ¸ë³´ë‹¤ ìµœì†Œ ì´ ë¹„ìœ¨ë§Œí¼ ì»¤ì•¼ ì œëª©ìœ¼ë¡œ ê°„ì£¼ (ë¹„ìœ¨)


def extract_section_title_by_font_heuristic(page: fitz.Page) -> str:
    """
    ì£¼ì–´ì§„ fitz.Page ê°ì²´ì—ì„œ í°íŠ¸ í¬ê¸° ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©í•˜ì—¬ ì„¹ì…˜ ì œëª©ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.
    ê°€ì¥ í° í°íŠ¸ í¬ê¸°ë¥¼ ê°€ì§„ í…ìŠ¤íŠ¸ë¥¼ ì œëª©ìœ¼ë¡œ ê°„ì£¼í•˜ë˜, ì¼ë°˜ í…ìŠ¤íŠ¸ì™€ ì¶©ë¶„íˆ êµ¬ë¶„ë  ë•Œë§Œ ì¸ì •í•©ë‹ˆë‹¤.
    """
    font_counts: Dict[float, int] = {} # í°íŠ¸ í¬ê¸°ë³„ ë¬¸ì ìˆ˜ ì¹´ìš´íŠ¸
    text_spans_by_size: Dict[float, List[Dict[str, Any]]] = {} # í°íŠ¸ í¬ê¸°ë³„ í…ìŠ¤íŠ¸ ìŠ¤íŒ¬ ì €ì¥

    blocks = page.get_text("dict", flags=fitz.TEXTFLAGS_TEXT)["blocks"]
    if not blocks:
        return "N/A"

    for block in blocks:
        if block["type"] == 0:  # Text block
            for line in block["lines"]:
                for span in line["spans"]:
                    size = round(span["size"], 2) # ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼í•˜ì—¬ ìœ ì‚¬ í°íŠ¸ ê·¸ë£¹í™”
                    text = span["text"].strip()
                    if not text: # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ë¬´ì‹œ
                        continue
                    
                    font_counts[size] = font_counts.get(size, 0) + len(text)
                    
                    if size not in text_spans_by_size:
                        text_spans_by_size[size] = []
                    text_spans_by_size[size].append({
                        "text": text,
                        "y": span["bbox"][1], # ì •ë ¬ì„ ìœ„í•œ y ì¢Œí‘œ
                        "x": span["bbox"][0]  # ì •ë ¬ì„ ìœ„í•œ x ì¢Œí‘œ
                    })

    if not font_counts:
        return "N/A"

    # ê°€ì¥ í”í•œ í°íŠ¸ í¬ê¸°ë¥¼ ë³¸ë¬¸ í…ìŠ¤íŠ¸ í¬ê¸°ë¡œ ê°„ì£¼ (ë¬¸ì ìˆ˜ ê¸°ì¤€)
    sorted_font_counts = sorted(font_counts.items(), key=lambda item: item[1], reverse=True)
    body_text_size = sorted_font_counts[0][0] if sorted_font_counts else 0.0

    # í˜ì´ì§€ì—ì„œ ê°€ì¥ í° í°íŠ¸ í¬ê¸° ì°¾ê¸°
    largest_font_size_on_page = 0.0
    if font_counts:
        largest_font_size_on_page = max(font_counts.keys())
    
    # ì œëª©ìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ íŒë‹¨
    # 1. í˜ì´ì§€ì— ë‹¤ì–‘í•œ í°íŠ¸ í¬ê¸°ê°€ ì‚¬ìš©ë˜ì—ˆê³ ,
    # 2. ê°€ì¥ í° í°íŠ¸ê°€ ë³¸ë¬¸ í°íŠ¸ë³´ë‹¤ ì˜ë¯¸ìˆê²Œ í´ ë•Œ
    is_title_plausible = (
        len(font_counts) > 1 and
        largest_font_size_on_page > body_text_size and
        (largest_font_size_on_page >= body_text_size + TITLE_FONT_SIZE_MIN_DIFFERENCE or
         largest_font_size_on_page >= body_text_size * TITLE_FONT_SIZE_MIN_RATIO)
    )

    if is_title_plausible and largest_font_size_on_page in text_spans_by_size:
        title_spans = text_spans_by_size[largest_font_size_on_page]
        # y, x ì¢Œí‘œ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í…ìŠ¤íŠ¸ ê²°í•©
        title_spans.sort(key=lambda s: (s["y"], s["x"]))
        page_section_title = " ".join(s["text"] for s in title_spans if s["text"])
        
        # ì œëª© ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°)
        if len(page_section_title) > 250:
            page_section_title = page_section_title[:250] + "..."
        return page_section_title
    
    return "N/A" # ê·¸ ì™¸ì˜ ê²½ìš° ì œëª©ì„ ì°¾ì§€ ëª»í•¨


def load_documents_from_dir(pdf_dir: str) -> List[Dict[str, Any]]: # Langchain DocumentëŠ” Dict[str, Any]ë¡œ í‘œí˜„ë  ìˆ˜ ìˆìŒ
    """í´ë” ë‚´ ëª¨ë“  PDF ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ê³  í°íŠ¸ í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ëœ ì„¹ì…˜ ì œëª©ì„ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€"""
    all_docs_with_metadata = []
    if not os.path.exists(pdf_dir) or not os.path.isdir(pdf_dir):
        print(f"âš ï¸  ê²½ê³ : PDF ë””ë ‰í† ë¦¬ '{pdf_dir}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
        return all_docs_with_metadata

    pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
    if not pdf_files:
        print(f"â„¹ï¸  ì •ë³´: PDF ë””ë ‰í† ë¦¬ '{pdf_dir}' ë‚´ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return all_docs_with_metadata

    print(f"ğŸ“‚ ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ ê°ì§€.")
    for pdf_path in pdf_files:
        file_name = os.path.basename(pdf_path)
        print(f"\nğŸ“„ '{file_name}' ë¡œë“œ ë° ì²˜ë¦¬ ì¤‘...")
        
        fitz_doc = None # fitz.Document ê°ì²´ ì´ˆê¸°í™”
        try:
            fitz_doc = fitz.open(pdf_path)
            
            # Langchainì˜ PyMuPDFLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ë³„ Document ê°ì²´ ìƒì„±
            loader = PyMuPDFLoader(pdf_path)
            docs_from_loader = loader.load() # í˜ì´ì§€ë³„ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            
            for i, lc_doc_page in enumerate(docs_from_loader):
                lc_doc_page.metadata['source_file'] = file_name # íŒŒì¼ëª… ë©”íƒ€ë°ì´í„° ì¶”ê°€
                
                if i < len(fitz_doc):
                    fitz_page = fitz_doc[i] # í˜„ì¬ í˜ì´ì§€ì˜ fitz.Page ê°ì²´
                    section_title = extract_section_title_by_font_heuristic(fitz_page)
                    lc_doc_page.metadata['section_title'] = section_title
                else:
                    # PyMuPDFLoaderê°€ ë¡œë“œí•œ í˜ì´ì§€ ìˆ˜ì™€ fitz_docì˜ í˜ì´ì§€ ìˆ˜ê°€ ë‹¤ë¥¼ ê²½ìš° ë°©ì–´ ì½”ë“œ
                    lc_doc_page.metadata['section_title'] = "N/A (í˜ì´ì§€ ì¸ë±ìŠ¤ ë¶ˆì¼ì¹˜)"
                    
            all_docs_with_metadata.extend(docs_from_loader)
            print(f"  '{file_name}' ë¡œë“œ ì™„ë£Œ. (í˜ì´ì§€ ìˆ˜: {len(docs_from_loader)})")

        except Exception as e:
            print(f"âŒ ì—ëŸ¬: '{file_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            if fitz_doc:
                fitz_doc.close() # fitz.Document ê°ì²´ ë‹«ê¸°
            
    return all_docs_with_metadata


def split_documents(
    docs: List[Dict[str, Any]], # Langchain Document
    chunk_size: int = CHUNK_SIZE,
    chunk_overlap: int = CHUNK_OVERLAP
) -> List[Dict[str, Any]]: # Langchain Document
    """ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  (ë©”íƒ€ë°ì´í„°ëŠ” ìƒì†ë¨)"""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""], # ë‹¤ì–‘í•œ êµ¬ë¶„ì ì‚¬ìš©
        length_function=len, # ë¬¸ìì—´ ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜
    )
    chunked_docs = splitter.split_documents(docs)
    print(f"âœ‚ï¸  ì´ {len(docs)}ê°œ ì›ë³¸ í˜ì´ì§€(ë¬¸ì„œ)ë¥¼ {len(chunked_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ.")
    return chunked_docs


def print_chunking_examples(chunked_docs: List[Dict[str, Any]], num_examples: int = 3, preview_length: int = 100):
    """ë¶„í• ëœ ì²­í¬ì˜ ì˜ˆì‹œë¥¼ ì¶œë ¥"""
    print(f"\nğŸ” ì²­í‚¹ ì˜ˆì‹œ (ì²˜ìŒ {num_examples}ê°œ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°):")
    if not chunked_docs:
        print("  (ë³´ì—¬ì¤„ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.)")
        return

    for i, chunk in enumerate(chunked_docs[:num_examples]):
        source_file = chunk.metadata.get('source_file', chunk.metadata.get('source', 'N/A'))
        page_number = chunk.metadata.get('page', 'N/A') # PyMuPDFLoaderê°€ 0-basedë¡œ ì¶”ê°€
        section_title = chunk.metadata.get('section_title', 'N/A')
        
        content_preview_raw = chunk.page_content[:preview_length]
        content_preview_processed = content_preview_raw.replace('\n', ' ')
        
        print(f"\n  --- ì²­í¬ {i+1} ---")
        print(f"  ì¶œì²˜ íŒŒì¼: {source_file}")
        print(f"  í˜ì´ì§€ ë²ˆí˜¸ (0-based): {page_number}")
        print(f"  ì¶”ë¡ ëœ ì„¹ì…˜ ì œëª©: {section_title}")
        print(f"  ë‚´ìš© (ì²˜ìŒ {preview_length}ì):")
        print(f"    \"{content_preview_processed}...\"")
        print(f"  (ì²­í¬ ê¸¸ì´: {len(chunk.page_content)}ì)")


def index_documents(docs: List[Dict[str, Any]], persist_dir: str):
    """ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  Chromaì— ì €ì¥"""
    try:
        embedding = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            # encode_kwargs={'normalize_embeddings': True} # í•„ìš”ì‹œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìœ„í•´ ì •ê·œí™”
        )
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: ì„ë² ë”© ëª¨ë¸ '{EMBEDDING_MODEL_NAME}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    print(f"âœ… ë¬¸ì„œ {len(docs)}ê°œ ì„ë² ë”© ì‹œì‘ (ëª¨ë¸: {EMBEDDING_MODEL_NAME})...")
    try:
        vectorstore = Chroma.from_documents(
            documents=docs, # Langchain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            embedding=embedding,
            persist_directory=persist_dir,
        )
        vectorstore.persist() # ë³€ê²½ì‚¬í•­ ë””ìŠ¤í¬ì— ì¦‰ì‹œ ì €ì¥
        print(f"âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {persist_dir}")
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: ë¬¸ì„œ ì„ë² ë”© ë˜ëŠ” Chroma DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    print("--- PDF ì„ë² ë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (í°íŠ¸ í¬ê¸° ê¸°ë°˜ ì„¹ì…˜ ì¶”ë¡ ) ---")

    print("\nğŸ“¦ PDF ë¡œë“œ ë° ë©”íƒ€ë°ì´í„°(ì„¹ì…˜ ì œëª©) ì¶”ì¶œ ì¤‘...")
    raw_docs_with_metadata = load_documents_from_dir(PDF_DIR)

    if not raw_docs_with_metadata:
        print("ğŸš« ë¡œë“œëœ PDF ë¬¸ì„œê°€ ì—†ì–´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    else:
        print(f"\nâœ‚ï¸  ë¬¸ì„œ ì²­í¬ ë¶„í•  ì¤‘ (ì²­í¬ í¬ê¸°: {CHUNK_SIZE}, ì¤‘ì²©: {CHUNK_OVERLAP})...")
        chunked_docs = split_documents(raw_docs_with_metadata)

        print_chunking_examples(chunked_docs) # ì²­í‚¹ ê²°ê³¼ ì˜ˆì‹œ ì¶œë ¥

        print("\nğŸ’¾ ë²¡í„° DB ì €ì¥ ì¤€ë¹„ ì¤‘...")
        # Chroma ë””ë ‰í† ë¦¬ ì¡´ì¬ ë° ë°ì´í„° ìœ ë¬´ í™•ì¸
        if os.path.exists(CHROMA_DIR) and os.listdir(CHROMA_DIR):
            choice = input(f"âš ï¸  ê²½ê³ : ë²¡í„° DB ë””ë ‰í† ë¦¬ '{CHROMA_DIR}'ì— ì´ë¯¸ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. \n    ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if choice == 'y':
                print(f"ğŸ—‘ï¸  ê¸°ì¡´ ë²¡í„° DB '{CHROMA_DIR}' ì‚­ì œ ì¤‘...")
                shutil.rmtree(CHROMA_DIR) # ë””ë ‰í† ë¦¬ì™€ ë‚´ìš© ëª¨ë‘ ì‚­ì œ
                os.makedirs(CHROMA_DIR, exist_ok=True) # ì‚­ì œ í›„ ë‹¤ì‹œ ìƒì„±
                print("ğŸ§  ì„ë² ë”© ë° ì €ì¥ ì‹œì‘...")
                index_documents(chunked_docs, CHROMA_DIR)
            else:
                print("ğŸš« ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ê¸°ì¡´ DBë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
        else:
            os.makedirs(CHROMA_DIR, exist_ok=True) # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            print("ğŸ§  ì„ë² ë”© ë° ì €ì¥ ì‹œì‘...")
            index_documents(chunked_docs, CHROMA_DIR)

    print("\n--- ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ---")
