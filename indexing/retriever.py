"""
ì‘ì„±ì : kp
ì‘ì„±ì¼ : 2025-05-18 (ìˆ˜ì •: 2025-05-20)
ëª©ì  : Chroma + BM25 ê¸°ë°˜ì˜ EnsembleRetriever êµ¬ì„± ë° ê²€ìƒ‰ ê²°ê³¼ ì¶œì²˜ í‘œê¸°
ë‚´ìš© : ì§€ì •ëœ PDF ë””ë ‰í† ë¦¬ì™€ Chroma DB ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ EnsembleRetrieverë¥¼ ìƒì„±.
       HuggingFaceEmbeddings ì„í¬íŠ¸ ê²½ë¡œë¥¼ LangChain 0.2.2+ ê¶Œì¥ ì‚¬í•­ì— ë§ê²Œ ìˆ˜ì •.
"""

import os
import glob
from typing import List, Optional

from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings # LangChain 0.2.2+
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

load_dotenv()

# ê¸°ë³¸ ì„¤ì •ê°’ (ì£¼ë¡œ ì§ì ‘ ì‹¤í–‰ ì‹œ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©)
DEFAULT_EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
DEFAULT_CHUNK_SIZE = 500
DEFAULT_CHUNK_OVERLAP = 100

def load_and_split_documents_for_bm25(
    pdf_dir: str,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP
) -> List[Document]:
    """BM25 Retrieverë¥¼ ìœ„í•œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í‚¹í•©ë‹ˆë‹¤."""
    docs_for_bm25 = []
    if not os.path.exists(pdf_dir) or not os.path.isdir(pdf_dir):
        print(f"âš ï¸  ê²½ê³  (BM25): PDF ë””ë ‰í† ë¦¬ '{pdf_dir}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return docs_for_bm25
        
    pdf_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
    if not pdf_files:
        print(f"â„¹ï¸  ì •ë³´ (BM25): PDF ë””ë ‰í† ë¦¬ '{pdf_dir}' ë‚´ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return docs_for_bm25

    raw_docs = []
    print(f"ğŸ“„ (BM25ìš©) '{pdf_dir}'ì—ì„œ ì´ {len(pdf_files)}ê°œ PDF íŒŒì¼ ë¡œë“œ ì¤‘...")
    for pdf_path in pdf_files:
        try:
            loader = PyMuPDFLoader(pdf_path)
            raw_docs.extend(loader.load())
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ (BM25): '{os.path.basename(pdf_path)}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if not raw_docs:
        return docs_for_bm25

    print(f"âœ‚ï¸  (BM25ìš©) ë¬¸ì„œ ì²­í¬ ë¶„í•  ì¤‘ (í¬ê¸°: {chunk_size}, ì¤‘ì²©: {chunk_overlap})...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", " ", ""], # ë‹¤ì–‘í•œ êµ¬ë¶„ì ì‚¬ìš©
        length_function=len,
    )
    docs_for_bm25 = splitter.split_documents(raw_docs)
    print(f"  (BM25ìš©) {len(docs_for_bm25)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ.")
    return docs_for_bm25


def build_ensemble_retriever(
    pdf_dir: str, # RAG ëŒ€ìƒ ë¬¸ì„œê°€ ìˆëŠ” ë””ë ‰í† ë¦¬
    chroma_persist_dir: str, # í•´ë‹¹ ì„œë¹„ìŠ¤ì˜ ChromaDB ì €ì¥ ê²½ë¡œ
    k_results: int = 3, # ê°€ì ¸ì˜¬ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
    bm25_weight: float = 0.4, # BM25 ê°€ì¤‘ì¹˜
    chroma_weight: float = 0.6, # Chroma ê°€ì¤‘ì¹˜
    embedding_model_name: str = DEFAULT_EMBEDDING_MODEL_NAME,
    chunk_size_for_bm25: int = DEFAULT_CHUNK_SIZE,
    chunk_overlap_for_bm25: int = DEFAULT_CHUNK_OVERLAP
) -> Optional[EnsembleRetriever]:
    """
    ì§€ì •ëœ ê²½ë¡œì˜ ë¬¸ì„œì™€ ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬ EnsembleRetrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        pdf_dir: BM25 ë° ì›ë¬¸ ì°¸ì¡°ë¥¼ ìœ„í•œ PDF ë¬¸ì„œê°€ ìˆëŠ” ë””ë ‰í† ë¦¬.
        chroma_persist_dir: ChromaDB ë°ì´í„°ê°€ ì €ì¥ëœ/ì €ì¥ë  ë””ë ‰í† ë¦¬.
        k_results: ê²€ìƒ‰ ì‹œ ë°˜í™˜í•  ê²°ê³¼ì˜ ìˆ˜.
        bm25_weight: EnsembleRetrieverì—ì„œ BM25 ê²°ê³¼ì˜ ê°€ì¤‘ì¹˜.
        chroma_weight: EnsembleRetrieverì—ì„œ Chroma ê²°ê³¼ì˜ ê°€ì¤‘ì¹˜.
        embedding_model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„.
        chunk_size_for_bm25: BM25ìš© ë¬¸ì„œ ì²­í‚¹ ì‹œ í¬ê¸°.
        chunk_overlap_for_bm25: BM25ìš© ë¬¸ì„œ ì²­í‚¹ ì‹œ ì¤‘ì²© í¬ê¸°.

    Returns:
        êµ¬ì„±ëœ EnsembleRetriever ê°ì²´ ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None.
    """
    print(f"ğŸ§  HuggingFace ì„ë² ë”© ë¡œë”© (ëª¨ë¸: {embedding_model_name})...")
    try:
        embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: ì„ë² ë”© ëª¨ë¸ '{embedding_model_name}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   (HINT: `pip install -U langchain-huggingface`ë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.)")
        return None

    print(f"ğŸ” Chroma ë²¡í„° ì €ì¥ì†Œ ë¡œë”© ì¤‘ (ê²½ë¡œ: {chroma_persist_dir})...")
    if not os.path.exists(chroma_persist_dir) or not os.listdir(chroma_persist_dir):
        print(f"âŒ ì—ëŸ¬: Chroma DB ë””ë ‰í† ë¦¬ '{chroma_persist_dir}'ê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print(f"   ë¨¼ì € '{pdf_dir}'ì˜ ë¬¸ì„œë¥¼ í•´ë‹¹ Chroma ê²½ë¡œë¡œ ì¸ë±ì‹±í•´ì•¼ í•©ë‹ˆë‹¤.")
        return None
        
    try:
        chroma_vectorstore = Chroma(
            persist_directory=chroma_persist_dir,
            embedding_function=embedding_model,
        )
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: Chroma DB ('{chroma_persist_dir}') ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

    semantic_retriever = chroma_vectorstore.as_retriever(search_kwargs={"k": k_results})
    print("  Chroma ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ.")

    print(f"ğŸ“„ BM25 ë¦¬íŠ¸ë¦¬ë²„ìš© ì›ë¬¸ ë¡œë”© ë° êµ¬ì¶• ì¤‘ (ì†ŒìŠ¤: {pdf_dir})...")
    bm25_docs = load_and_split_documents_for_bm25(
        pdf_dir,
        chunk_size=chunk_size_for_bm25,
        chunk_overlap=chunk_overlap_for_bm25
    )
    
    lexical_retriever = None
    if not bm25_docs:
        print("âš ï¸  ê²½ê³ : BM25 ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìœ„í•œ ë¬¸ì„œê°€ ì—†ì–´ BM25ëŠ” ì œì™¸í•˜ê³  Chroma ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        try:
            lexical_retriever = BM25Retriever.from_documents(bm25_docs)
            lexical_retriever.k = k_results
            print("  BM25 ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ ì—ëŸ¬: BM25 ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if lexical_retriever and semantic_retriever:
        print("ğŸ”— EnsembleRetriever êµ¬ì„± ì¤‘ (Chroma + BM25)...")
        ensemble_retriever = EnsembleRetriever(
            retrievers=[semantic_retriever, lexical_retriever],
            weights=[chroma_weight, bm25_weight],
        )
    elif semantic_retriever:
        print("ğŸ”— Chroma ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš© (BM25 ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë¬¸ì„œ ì—†ìŒ).")
        ensemble_retriever = semantic_retriever # BM25ê°€ ì—†ìœ¼ë©´ Chromaë§Œ ì‚¬ìš©
    else:
        print("âŒ ì—ëŸ¬: Semantic retriever (Chroma)ë„ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Retrieverë¥¼ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None


    print("âœ… ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„± ì™„ë£Œ.")
    return ensemble_retriever

if __name__ == "__main__":
    print("--- Ensemble Retriever ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---")
    
    # ì´ í…ŒìŠ¤íŠ¸ëŠ” íŠ¹ì • ì„œë¹„ìŠ¤ì˜ ë°ì´í„° ë””ë ‰í† ë¦¬ì™€ Chroma DB ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    # ì˜ˆ: test_pdf_dir = "./data/daglo"
    # ì˜ˆ: test_chroma_dir = "./vectorstore/chroma_daglo"
    # ìœ„ ê²½ë¡œì— ëŒ€í•´ indexer.pyê°€ ë¨¼ì € ì‹¤í–‰ë˜ì–´ DBê°€ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

    test_pdf_dir = "./data/daglo" # í…ŒìŠ¤íŠ¸í•  PDF ë¬¸ì„œê°€ ìˆëŠ” í´ë”
    service_name_for_db = os.path.basename(test_pdf_dir) # 'daglo'
    test_chroma_dir = f"./vectorstore/chroma_{service_name_for_db}" # í…ŒìŠ¤íŠ¸í•  Chroma DB ê²½ë¡œ

    print(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ PDF í´ë”: {test_pdf_dir}")
    print(f"í…ŒìŠ¤íŠ¸ ëŒ€ìƒ Chroma DB í´ë”: {test_chroma_dir}")

    if not (os.path.exists(test_pdf_dir) and os.path.exists(test_chroma_dir) and os.listdir(test_chroma_dir)):
        print(f"\nğŸš« í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨: í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ PDF í´ë” ë˜ëŠ” Chroma DBê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print(f"   '{test_pdf_dir}'ì— PDF íŒŒì¼ì´ ìˆëŠ”ì§€,")
        print(f"   '{test_chroma_dir}'ì— í•´ë‹¹ PDFì— ëŒ€í•œ ì¸ë±ì‹±ëœ Chroma DBê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   (HINT: python indexing/indexer.py --service_data_dir {test_pdf_dir} ì™€ ê°™ì´ ì‹¤í–‰í•˜ì—¬ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš”.)")
    else:
        retriever = build_ensemble_retriever(
            pdf_dir=test_pdf_dir,
            chroma_persist_dir=test_chroma_dir,
            k_results=3
        )

        if retriever:
            query = "Daglo AI Guideì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€?"
            print(f"\nğŸ’¬ ê²€ìƒ‰ì–´: \"{query}\"")
            
            try:
                results = retriever.invoke(query) 
            except AttributeError: 
                 results = retriever.get_relevant_documents(query)

            print(f"\nğŸ“Œ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):")
            if not results:
                print("  ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            for i, doc in enumerate(results):
                content_preview = doc.page_content[:200].replace("\n", " ") + "..."
                source_file = doc.metadata.get('source_file', doc.metadata.get('source', 'N/A'))
                page_num = doc.metadata.get('page', 'N/A') 
                section_title = doc.metadata.get('section_title', 'N/A')

                print(f"\n--- ê²°ê³¼ {i + 1} ---")
                print(f"  ğŸ“– ë‚´ìš© (ì¼ë¶€): \"{content_preview}\"")
                print(f"  ğŸ“„ ì¶œì²˜ íŒŒì¼: {source_file}")
                print(f"  - í˜ì´ì§€: {page_num if page_num != 'N/A' else 'ì •ë³´ ì—†ìŒ'} (0-based)")
                print(f"  - ì¶”ë¡ ëœ ì„¹ì…˜: {section_title if section_title != 'N/A' else 'ì •ë³´ ì—†ìŒ'}")
        else:
            print("ğŸš« ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±ì— ì‹¤íŒ¨í•˜ì—¬ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n--- í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ---")
